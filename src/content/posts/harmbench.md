---
title: 通用红队基准 HarmBench
published: 2025-06-28
description: "ICML 2025 Paper \"HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal\""
tags: ["Jailbreak", "Benchmark"]
category: Paper Reading
draft: false
---

:::note[TL;DR]
本文提出 HarmBench，一个专为自动化红队攻击与鲁棒拒绝机制评估设计的标准化基准，系统性定义了红队评估的三个关键维度：广度、可比性与鲁棒性。HarmBench 收录了 510 个高质量的有害行为样本，涵盖文本、多模态、上下文与版权场景，首次实现了对 18 种攻击方法和 33 个 LLM 的大规模标准对比。该基准揭示了当前攻击与防御方法的局限，并为未来攻防协同开发提供了统一实验平台。
:::


# Background & Motivation

为了防止 LLMs 被恶意使用，许多领先的开发人员采用了各种实践策略和防御措施来应对恶意使用，其中红队操作是其中的关键组成部分，因为它能使公司能够在部署前发现并修复其防御中的漏洞。然而，目前公司主要依赖手动红队操作，但手动红队操作无法探索 AI 可能遇到的所有对抗性或长尾场景，因此开发自动化红队操作方法以评估和强化防御引起了广泛关注。

虽然近期一些红队技术采用了自动化的处理，但是这些攻击采用了不同的评估方法，使得这些红队技术难以比较。此外，作者发现先前的评估缺乏准确评估自动化红队技术所需的重要理想特性。为了解决这些问题，文章引出了一个新的红队攻防基准，名为 HarmBench，并确定了红队评估的三个理想特性：广度、可比性和鲁棒性。并且，作者提出了一个新的鲁棒拒绝对抗训练方法，名为鲁棒拒绝动态防御（R2D2）。


# Method

作者首先讨论了评估红队方法时必须具备的三个关键特性：广度、可比性与鲁棒性。所谓广度，是指评估体系需要覆盖尽可能多样的有害行为场景，包括不同类型的语言结构、语境信息甚至是多模态输入，才能真实反映攻击方法的适用范围与模型防御的盲区。而可比性则要求评估过程中的所有参数与设置应保持统一，否则即便使用相同攻击方法，不同论文间的评估结果也无法对齐，作者甚至发现仅仅改变解码 token 长度就能使攻击成功率产生高达 30% 的波动。最后，鲁棒性则强调评估标准本身应抗干扰、不易被攻击者“投机取巧”，这就要求用于判断攻击是否成功的分类器必须应对多种非标准输入情况并保持准确性。

基于这些理念，作者提出了 HarmBench 框架，并重新定义了红队评估的标准。HarmBench 包含 510 个有害行为，被细分为四类功能类别：标准行为、上下文行为、版权行为以及多模态行为。其中标准行为基本模仿已有红队数据集，版权行为专注于诱导模型输出受版权保护内容，而上下文行为和多模态行为则是结构上最具挑战性的类别，前者引入了人物背景等信息，使请求更具诱导性与隐蔽性，后者则融合图像与文本，考验模型在跨模态场景下的安全防线。进一步地，作者提出“在线可搜索性”这一标准，**将原本对“危害”的定义从“是否违法”转向了“是否比人类更高效地生成有害信息”。例如一些毒品合成流程本可在学术论文中找到，但若模型能够根据家庭条件与隐蔽性优化合成路径，那就属于具有明显“差异性危害”的行为。与此同时，作者也明确指出许多现有数据集中存在“dual-intent”问题，即请求行为本身可能同时具备合法与非法用途，例如编写加密函数既可能是软件开发，也可能用于勒索软件，这些模糊样本在 HarmBench 的构建过程中被系统性地筛除或重写，以保证训练与评估更加纯净与稳定。**

在评估流水线设计中，HarmBench 采用了三阶段流程，包括攻击生成、模型输出生成以及分类器评估。攻击方法首先根据有害行为生成输入 prompt，随后传入目标模型生成 512 token 的响应，最后由分类器判断模型是否完成了指定有害行为。在分类器设计上，非版权行为由作者使用 LLaMA-2-13B Chat 模型微调而成，该模型在人工标注的验证集上显著优于现有开源和闭源方法，且具备极强鲁棒性。而版权行为因不便人工直接判断，作者设计了一个基于哈希的比对系统，将生成内容与已知版权材料进行不可逆比对，借此判断是否构成实质性重现。

为进一步探索如何将攻击结果用于反向增强防御，作者提出了 R2D2，一种鲁棒拒绝的动态对抗训练方法。与传统方法使用静态 harmful prompt 不同，R2D2 在训练过程中持续使用 GCG 攻击方法生成更新的 adversarial prompt，并将这些 prompt 与固定拒绝语句（如“对不起，我无法满足该请求”）配对，构建训练样本池。在每轮训练中，作者随机选取一部分 prompt 进行多步优化，并引入双重损失设计，其中一项损失鼓励模型远离原始目标内容，另一项则直接优化拒绝语句的输出概率，同时混合标准指令调优任务避免模型能力塌缩。此外，R2D2 设计了定期样本池重置机制，以维持多样性并避免训练过程陷入局部最优。这种结构使模型不只是“学会说不”，而是能在遭遇复杂引导、上下文绕行或误导性开头时主动回归安全轨道，体现出更深层次的对齐能力。


# Experiment

作者基于 HarmBench 对 18 种红队攻击方法和 33 个 LLM 进行了全面测试，涵盖开源模型（如 LLaMA、Mistral、Vicuna、Zephyr、Gemma 等）与部分闭源模型（如 GPT-4、Claude、Gemini 等）。**测试结果表明当前没有一种攻击方式可以对所有模型都保持高成功率，也没有一个模型可以在所有攻击方式下保持强鲁棒性，防御与攻击之间呈现典型的非对称分布，这再次验证了使用单一攻击作为训练参考的策略在真实对抗场景中存在显著局限。**同时，实验还揭示了一个长期被误解的现象，**即模型规模并不是鲁棒性的关键因素，同一系列中从 7B 到 70B 的鲁棒性并无显著变化**，反而是训练方法、对齐策略、系统 prompt 和训练数据质量决定了模型面对攻击时的表现差异。特别是在上下文行为与多模态行为这两类更具现实威胁的任务中，当前主流模型表现普遍较差，说明大多数防御方法仍停留在结构简单、信息密度较低的行为层面上，难以应对更隐蔽的攻击策略。

针对 R2D2 方法的测试也展现了极具参考价值的结果。作者在 Zephyr 7B 模型上以 R2D2 策略进行微调训练，仅用 256 个持续优化的攻击样本，就使模型在 GCG、GCG(Multi) 与 GCG(Transfer) 三种攻击下的平均成功率降至 5.9%，远低于原始 Zephyr 模型或 LLaMA2 13B 这类现有主力对话模型的 30% 以上表现。同时，在常规能力评估 MT-Bench 上，该模型得分基本维持在原有水平，说明 R2D2 不仅在安全性上带来了实质性收益，而且避免了传统对抗训练中常见的 utility collapse。值得注意的是，当面对训练外攻击方法（如 TAP 与 PAIR）时，R2D2 模型虽然仍具提升，但效果不如在 GCG 上显著，这一现象提示未来的对抗训练需要融合多种攻击策略才能获得泛化性更强的鲁棒防线。


# Rating

如果从“评估范式标准化”这个角度来看，HarmBench 绝对是红队领域的重要突破，它填补了此前方法难以复现、难以横向比较的问题。虽然这篇文章在提出的方法 R2D2 在 over-refusal 的权衡、以及防御泛化性（如应对 TAP/PAIR 等非 GCG 攻击）上仍有改进空间，但整体贡献无疑具有代表性。

