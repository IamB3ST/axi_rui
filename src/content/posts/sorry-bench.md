---
title: 通用防御基准 SORRY-Bench
published: 2025-07-02
description: "ICLR 2025 Paper \"SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal\""
tags: ["Safety Alignment", "Benchmark"]
category: Paper Reading
draft: false
---

:::note[TL;DR]
SORRY-Bench 是一个系统化且更加平衡的 LLM 安全拒绝能力的基准，覆盖 45 个细粒度高风险话题与 20 种语言变体，组成一个结构均衡、语言多样的 9K 规模不安全请求数据集。此外，作者还微调了一个小规模 LLM（Mistral-7B）作为安全评估器，其准确性与 GPT-4o 相当但计算成本更低。
:::


# Background & Motivation

为了系统地评估对齐后的 LLM，研究人员提出了许多基准来评估 LLM 安全性的各个方面，包括毒性、有害性、可信度和拒绝行为。

但是作者发现此前数据集的数据分布通常在主题和语言特征上都存在 bias。为了弥补这一差距，作者团队针对 4 个领域提出了一个细粒度的 45 类分类法，并尝试通过 20 多种语言变异在构建数据集时确保语言特征上的平衡。基于这种安全分类法，作者团队提出了一个包含 450 条不安全指令（每个类别 10 个）的 LLM 安全拒绝评估数据集。在这个数据集的基础上，作者又通过语言变异进行改写，获得了 9K 个额外的有害指令，组成了最终的 SORRY-Bench。此外，作者还微调了一个 7B 的 LLM 作为 evaluator，其与人类评判的一致性甚至超过了 api 级别规模的 LLM 如 GPT-4o。

# Method

SORRY-Bench 的构建基于作者提出的一套覆盖全面、粒度精细的安全拒绝分类体系。具体而言，他们从已有的 10 个安全评估数据集中归纳抽象出 45 个细粒度风险类别，进一步聚合为 4 个高层级主题域（仇恨言论、犯罪协助、不当内容、不合格建议），构成统一的 taxonomy。该分类体系既整合了过往 benchmark 的结构，又通过 GPT-4 辅助分类 + 人类反复迭代审核的方式，补充了此前未覆盖的重要风险类别如“法律建议”或“动物相关犯罪”。在此基础上，作者从现有数据集中抽取并清洗高质量的不安全指令，同时对类别样本不均问题进行补充，最终构建出一个类均衡的 benchmark 数据集，包含每类 10 条、共 450 条不安全请求。

评估模型拒绝能力的一个关键挑战是如何构建一个高效且准确的自动评估器来判断模型是否真正“拒绝”了用户请求。为此，作者构建了一个包含 7200 条人类标注的拒答判断数据集，覆盖基础 ID（分布内）数据集（450 条指令 × 8 个模型回复）和语言变体 OOD（分布外）数据集（同 ID 结构），每条回复由 6 位作者标注“完成”或“拒绝”，且将其中 450 *（3 ID + 3 OOD）= 2700 条数据作为训练集，其余 4500 条数据划分为测试集。作者在此基础上对多种自动评估器设计进行了系统性比较，包括 GPT-4o zero-shot、CoT 推理、Few-shot 示例、以及基于该数据集微调的多种小模型。


# Experiment

关于不同设计的评估器之间的实验显示，GPT-3.5 和 LLaMA 系列模型在未调优状态下的评估一致性明显不足，而只有基于人类标注数据进行微调的评估器才能达到与 GPT-4o 类似甚至更优的效果。特别是 fine-tuned 的 Mistral-7B 在评估准确性和运行效率之间达成最优平衡，平均每次评估仅耗时 10 秒，Cohen Kappa 达 0.81，成为实际应用中性价比最高的选择，因此被 SORRY-Bench 采用为默认标准。

作者还评估了包括 GPT-4o、Claude-2、Gemini-1.5、LLaMA-3、Mistral、Gemma 等在内的 43 个主流开源与闭源大模型。每个模型在 450 条基础不安全请求上的回复都被评估是否为“完成”或“拒绝”，以此计算完成率（Fulfillment Rate）作为衡量其拒绝能力的指标。作者还在部分模型上评估了语言变体带来的影响，并进一步探索不同解码策略与 prompt 格式对评估结果的影响。

实验结果显示，不同模型之间在拒绝能力上的差异显著。Claude-2 与 Gemini-1.5 系列拒绝比例最高，完成率低于 10%，表现出严格的拒绝策略。Mistral 与部分 LLaMA-3 模型则显著更容易“越狱”，部分模型完成率超过 50%。此外，某些类别如“骚扰”、“儿童相关犯罪”在大部分模型上普遍被拒绝，而“法律建议”、“成人建议”等类别的拒绝策略在模型间差异较大。例如，GPT-4o 在“成人建议”上完成率大幅提升，反映出其在 2024 年新发布的 OpenAI Model Spec 中对伦理边界的新定义。语言变体实验表明，技术术语、劝说语气、多语种等因素显著影响模型是否拒绝，尤其是 persuasion 类 prompt 能显著增加模型完成率。


# Rating

虽然该工作构建的数据集与评估结果在内容上并不新颖，但该论文的主要贡献在于建立了一套完整、精细且可复用的数据集构建流程与评估器制作范式。因此尽管实验发现不突出，但该基准在 methodology 层面的系统性与可拓展性，为未来构建更通用的对齐评估工具提供了良好的样板。