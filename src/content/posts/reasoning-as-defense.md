---
title: 推理作为防御手段
published: 2025-08-01
description: "ICML 2025 PUT Workshop \"Reasoning as an Adaptive Defense for Safety\""
tags: ["Alignment", "Post-training", "Reasoning"]
category: Paper Reading
draft: false
score: 4
---

:::note[TL;DR]
本文提出了一种名为 TARS (Training Adaptive Reasoners for Safety) 的方法论，旨在通过 RL 训练模型利用推理能力来动态应对安全风险。该方法的核心是让模型在响应前生成 CoT，并根据一个平衡了安全性与任务完成度的奖励信号进行优化。研究发现，TARS 能够让模型在面对模糊或复杂的请求时，自适应地投入更多计算资源进行推理，从而在“安全-拒答”的权衡中取得更优表现。与传统的 SFT 或 DPO 相比，TARS 训练出的模型不仅在内部表征上能更好地区分有害与无害提示，而且对白盒和黑盒攻击都表现出更强的鲁棒性。
:::


# Background & Motivation

在 LLM 的研发进程中，一个显著的趋势是利用增加测试时计算（test-time compute）来提升模型在复杂问题上的推理能力，这一范式在数学、代码等易于验证的领域已取得巨大成功。其核心特征在于，模型能够根据问题的预期难度，自适应地决定投入多少计算资源进行思考。这种动态的、深思熟虑的解决问题方式，似乎天然地契合了AI安全领域所面临的挑战。安全问题，尤其是如何应对那些精心设计的、意图模糊或复杂的有害请求，同样需要超越简单的模式匹配，进行更深层次的审慎判断。因此，一个自然而然的问题浮出水面：我们是否能将这种自适应推理的范式迁移到安全防御中，让模型通过增加“思考”来更有效地识别和拒绝有害请求，同时避免 over-refusal ？

尽管一些研究已经开始探索利用推理来增强模型的安全性，但现有工作要么细节不详，要么主要依赖于 SFT 等静态方法，缺乏一个系统性的、关于如何有效训练 LLM 进行安全推理的最佳实践或“配方”。这留下了一系列悬而未决的关键问题：我们应该如何设计训练数据？SFT 和 RL 哪种范式更优，或者应该如何结合？在 RL 训练中，模型是否会学到“一概拒绝”之类的捷径（shortcut behavior），如果会，又该如何缓解？本文正是为了回答这些问题，构建了一个名为TARS（Training Adaptive Reasoners for Safety）的 RL 框架。作者们通过系统性的实验，识别出训练安全自适应推理模型的三个关键设计要素，并证明了通过引导LLM在测试时进行推理，确实可以在安全性和可用性之间达成更好的平衡，尤其是在处理复杂和模糊的提示时。

# Method

TARS 的核心是一种基于 RL 的 post-training 配方，它使用长 CoT 和精心设计的混合数据及奖励函数，旨在教会模型在回应前先进行自适应的安全推理。整个流程被巧妙地设计为三个环环相扣的阶段。

第一阶段是轻量级的监督微调（Lightweight SFT）。此阶段的目标并非直接教会模型完美的安全性，而是为其后续的RL探索阶段“热身”。研究者们发现，如果直接从一个普通的指令微调模型开始RL，模型很难学会有效的安全推理。因此，他们首先使用一个更强大的模型（如 DeepSeek-R1）针对一批有害提示生成多样化的、包含`<think>...</think>`标签的推理轨迹。这些轨迹不必是完美的，其关键在于为目标模型（本文中为Qwen-2.5-1.5B-Instruct）提供推理的“结构”和“格式”范例。通过在一个较低的学习率下进行短时间的 SFT，模型得以初步掌握安全推理的基本行为模式，同时又不会因为过度拟合而丧失探索多样性的能力，这一点对于后续 RL 阶段的成功至关重要。

第二阶段是为强化学习精心设计提示集。单纯在有害提示上进行训练，很容易导致模型学会“万能拒答”的捷径，从而在无害问题上也丧失功能。为了解决这个问题，TARS 采用了一种混合提示策略。它不仅包含来自 WildJailbreak 等数据集的有害提示，还引入了大量来自 UltraFeedback 的无害提示，以及一部分从 OR-Bench 中筛选出的“模糊”提示。这些模糊提示的特点是，它们本身无害，但容易被安全模型误判为有害。这种有害、无害、模糊三者混合的数据构成，迫使模型必须进行真正的推理来区分不同情况，而不是依赖简单的启发式规则，从而有效避免了在学习安全性的同时退化通用推理能力。

第三阶段是设计分离式的强化学习奖励函数。这是 TARS 的又一个关键创新。研究者发现，试图用一个统一的奖励模型（如通用的偏好奖励模型 GRM）来同时评估安全性和帮助性，效果并不理想。取而代之，他们设计了两个独立的奖励函数：一个“安全奖励”和一个“任务完成奖励”。对于有害提示，模型的响应会由 OpenAI 的 Moderation API 进行打分，分数越低（即越安全），奖励越高。而对于无害和模糊的提示，则使用一个开源的 GRM 来评估其回答的帮助性和任务完成度。此外，还有一个二进制的“格式奖励”，以确保模型正确生成了`<think>`标签。最终的总奖励是格式奖励与上述两个情境奖励的乘积。这种分离式的奖励设计，使得优化目标更加清晰，让模型能够在有害提示上专注于提升安全性，在无害提示上专注于提升帮助性，从而在整体上实现了更优的“安全-拒答”帕累托前沿。

# Experiment

为了验证 TARS 的有效性，论文进行了一系列详尽的实验和对比。首先，研究者将 TARS 与多种基线方法（包括 SFT、DPO 以及不带推理的纯 RL）在相同的混合数据比例下进行了训练和比较。结果清晰地显示，在“安全-拒答”的权衡曲线上，TARS 全面优于所有其他方法。这表明，无论是推理能力还是 RL 范式，对于实现最佳的安全性能都是不可或缺的。有趣的是，不带推理的RL虽然在安全性上强于 SFT，但 TARS（带推理的RL）的表现则更胜一筹，证明了推理过程本身为安全决策提供了关键的增益。

其次，论文将 TARS 与当前顶尖的开源安全模型及防御方法进行了比较，例如 Llama-3 系列模型和使用 Representation Re-routing 技术的 Circuit Breaker 模型。尽管 TARS 所使用的基础模型（1.5B参数）远小于这些对手（如8B参数的Llama），但它依然在安全-拒答的权衡中取得了极具竞争力的、甚至在某些方面更优的结果。这有力地证明了TARS配方的有效性，即一个相对较小的模型，在经过高效的推理训练后，其安全性能可以超越比它大数倍但未使用类似训练策略的模型。

为了探究 TARS 为何有效，研究者进一步分析了模型在面对不同复杂度提示时的行为。通过在 Sorry-Bench 上进行测试，他们发现 TARS 训练的模型展现出了显著的“自适应”特性：对于那些意图明确的有害提示（如“仇恨言论”），模型的推理链相对较短，能快速做出拒绝决策；而对于那些更复杂、更模糊的提示（如“可能不合规的建议”），模型则会生成长得多的推理链，投入更多的计算资源进行审慎分析。这种自适应的计算资源分配，正是TARS能够在不牺牲过多帮助性的前提下提升安全性的关键。最后，通过使用 UMAP 对模型最后一层的表征进行降维可视化，研究者发现，相比 SFT、DPO 或纯 RL，TARS 能够将有害攻击提示（GCG）和无害模糊提示（XSTest）的内部表征在空间上分得最开。这说明 TARS 不仅在行为上学会了区分，更在模型的内部几何结构中形成了对安全概念更清晰、更鲁棒的表征，这或许是其能够更有效抵御攻击的深层原因。

# Rating

TARS 这项工作为对齐领域贡献了一个具有高度实践指导意义的“配方”。它最核心的价值在于，没有止步于“推理对安全有用”这一笼统的论断，而是通过一系列严谨的消融实验和对比分析，系统性地拆解了“如何有效训练一个会思考的安全模型”这一复杂问题。然而，论文在论证其核心贡献时也存在一些模糊之处，使得读者难以完全厘清其成功的关键归因。例如，整个方法论高度依赖于一个强大的“教师模型”（DeepSeek-R1）来生成初始 SFT 阶段的推理轨迹，但论文并未提供消融实验来探究不同教师模型或无教师模型时的表现，这使得 TARS 配方的普适性存疑。