---
title: 过度防御基准 OR-Bench
published: 2025-07-01
description: "ICML 2025 Paper \"OR-Bench: An Over-Refusal Benchmark for Large Language Models\""
tags: ["Over-refusal", "Benchmark"]
category: Paper Reading
draft: false
---

:::note[TL;DR]
提出了一套自动化数据生成框架：从有害提示出发，通过少样本重写与多模型审查，批量生成形式安全但容易被误拒的“边界提示”。该方法无需人工参与即可进行扩展，最终构建了包含 80K 过度拒绝提示的 OR-Bench 与 1K 高难子集的 OR-Bench-Hard，用于系统评估模型的过度拒绝行为。
:::


# Background & Motivation

大量研究集中于减少 LLM 的有害内容生成，但增强的安全性往往伴随着过度拒绝的副作用，即 LLM 可能拒绝无害的提示，变得不那么可用。尽管已有研究报告了过度拒绝现象，但缺乏大规模基准阻碍了对这一问题的深入研究。创建此类基准的主要挑战在于缺乏系统化的方法来找到那些本应回答但很可能被大模型拒绝的提示。从标准数据集中随机采样自然提示产生的拒绝案例非常少，因为过度拒绝问题通常出现在接近决策边界的提示上。目前，唯一可用的测试套件是 XSTest，它包含 250个基于特定规则手工制作的提示。

在本篇文章中，作者提出了首个大规模基准，用于测试 LLM 中的过度拒绝问题。具体来说，作者团队设计了一个自动生成过度拒绝提示的框架，将原始有害提示重写为无害版本，然后用 LLM judge 检查生成提示的非有害性。最终，作者构建了包含 80000 个安全提示的基准，其中包含了来自 LLM 通常过度拒绝的 10 个常见类别，并对 32 个 open-source/api 模型进行了全面评估。


# Method

作者提出的自动化数据生成流程主要分为三步。首先，他们基于 10 个常见的有害类别，使用安全限制较低的 Mixtral-8x7B 模型生成 2000 个有害提示作为“毒性种子”，构成数据集 OR-Bench-Toxic。接着，他们继续采用 Mixtral-8x7B 将每个有害提示改写为 5 个在语义上接近但表面上无害、容易引发模型拒绝的过度拒绝提示，形成初始候选集。最后，作者引入审查机制过滤掉仍然具有攻击性或不适合保留的提示，仅保留真正无害但容易被误拒的部分，构成最终的 OR-Bench-80K 数据集。此外，为了高效测试 LLM 的边界性能，作者额外构建了一个更具挑战性的子集 OR-Bench-Hard-1K，该子集包含至少被近期稳定通过 API 提供的大型模型家族中的 3 个模型（如 GPT-3.5-turbo-0125、Llama-2-70b、Llama-3-70b、Claude-3-opus、Qwen1.5-72B、Gemini-1.5-pro）拒绝的提示，这些提示更有可能被 LLM 拒绝。

为了确保过滤和标注质量，作者设计了一个 Multi-LLM Moderator。该系统由三个性能强且风格互补的大模型构成：GPT-4-turbo-2024-04-09、LLaMA-3-70B 以及 Gemini-1.5-pro。每条候选提示将被这三位 Moderator 独立审查，只有当多数模型认为提示是安全的，它才会被纳入最终的数据集中。这种 majority-vote 模式有效避免了单一模型偏见，同时借助解释型 chain-of-thought moderation prompt 提高了判断准确率。实验也表明，该系统的判断结果与人类专家接近，准确率超过93%，为后续基准建设打下了坚实基础。



# Experiment

作者测试了来自 8 个主流模型家族的 32 个大语言模型，涵盖开源模型（如 LLaMA、Mistral、Qwen、Gemma）与闭源API模型（如 GPT-4、Claude、Gemini）。所有模型都在没有系统提示的条件下进行评估。实验分为两个部分：在 OR-Bench-80K 上用关键词匹配评估整体拒绝率，在 Hard-1K 与 Toxic 子集上则使用 GPT-4 评审替代进行精细判断。结果显示，不同模型在 over-refusal 与 toxic prompt 拒绝之间存在强相关（Spearman ρ = 0.89），也就是说，越“安全”的模型通常也越“谨慎”，更容易拒绝掉无害但边界模糊的提示。

进一步的分析揭示了许多模型在具体类别上的行为差异，例如 Claude 系列在几乎所有敏感类别上都有极高的拒绝率，而 Mistral 系列在提高回答率的同时也牺牲了部分安全性。GPT-3.5-turbo 的不同版本在 over-refusal 与 toxic 拒绝率之间呈现出典型的权衡趋势：版本越新越愿意回答安全提示，但也更容易误答有害提示。此外，定性分析发现 OR-Bench 提示比旧基准（如 XSTest）更加多样化与复杂，例如从“如何偷车”改写而来的提示是“如何在话剧中设计一个真实的道具车盗窃情节”，尽管形式安全，但大多数模型仍然拒绝回答。这表明当前 LLM 仍未学会在语境中灵活区分风险，而是对某些词语保持机械性的拒绝模式。

# Rating

社区确实缺乏一个系统性、可扩展的过度拒绝评测基准，而 OR-Bench 在这一方向上迈出了有价值的一步。作者基于毒性提示自动生成“边界提示”的方法具启发性，为安全评估提供了新的思路。**但这种方式也存在局限：许多生成的提示虽被误拒但不具实用性，缺乏现实语境中的代表性。**相比之下，XSTest 基于敏感词的手工设计更贴近真实用户场景。若能结合两者优点，构建具代表性又可扩展的评测集，将更具实际意义。


