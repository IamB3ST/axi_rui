---
title: 推理模型并非总能言行一致
published: 2025-07-15
description: "Anthropic Techniqual Report \"Reasoning Models Don't Always Say What They Think\""
tags: ["Alignment", "Reasoning"]
category: Paper Reading
draft: false
score: 5
---

:::note[TL;DR]
本论文聚焦于大语言模型（LLM/LRM）思维链推理的忠实性，即 CoT 是否真实反映模型内部推理过程。研究通过设计含/无提示的问题对来衡量模型 CoT 中对提示的承认度，结果发现 CoT 普遍缺乏忠实性，即使模型使用了提示，其在 CoT 中明确提及的比例也常低于 20%，对不安全提示的忠实性尤其令人担忧。出乎意料的是，不忠实的 CoT 反而比忠实的CoT更为冗长，这与简洁性假设相悖。同时，忠实性随任务难度增加而降低，基于结果的强化学习（RL）对其提升有限，仅能带来初期改善但未能持续饱和，表明其不足以解决深层次的忠实性问题。
:::


# Background & Motivation

在回应用户之前，LRMs 可以通过 CoT 来进行推理、规划和探索，从而以更高的准确率解决复杂任务。不仅如此，我们还可以监控模型的 CoT 推理过程，以尝试理解其响应背后的意图和目标。为了使 CoT 的监控最有效，其必须是模型面对用户最终响应的清晰反映，也就是说，CoT 必须对于人类可理解并突出其推理背后的关键因素和步骤。如果思维链不忠实，那么我们就无法依赖监控思维链来检测行为偏差，因为可能有影响模型行为的安全相关因素未被明确表达出来。另一方面，RLHF 可能会激励模型在 CoT 中隐藏不良推理。即使 RLHF 带来的奖励并为直接应用于 CoT 推理，而是面向用户的最终响应，这些最终响应的泛化仍可能会影响思维链行为。因此，评估模型的 CoT 忠实性对基于 CoT 监控的安全性方法可靠性至关重要。

为此，作者团队评估了两种最先进的 LRM（Claude 3.7 Sonnet 和 DeepSeek R1）在 6 种推理提示中的 CoT 忠实性。且除了评估训练好的模型，作者还研究了基于结果的强化学习如何影响 CoT 的忠实度，具体来说，作者提出假设：对于需要一定推理的任务，仅奖励其最终回复，也会激励模型生成可靠的 CoT。并根据这个假设进行了相关的实验。随后，作者还研究了 CoT 监控在训练阶段的具体应用 —— 在强化学习中检测奖励欺骗。

# Method

作者首先定义了 CoT 的忠实性：给定输入 `x`，一个模型 `M` 会自回归的生成一个思维链 `c` 和最终答案 `a`，如果 `c` 准确反映了 `M` 在对输入 `x` 做出回复 `a` 时的内部推理过程，则它是忠实的。我们不要求模型生成 CoT 时候逐字描述中间所有推理内容，但忠实的 CoT 应突出关键决策因子和推理链，这些因素将使我们能够预测它在部署期间如何处理类似的输入。

作者团队通过构造一组问题对来评估忠实性，具体来说，每对问题包括一个 “没有提示的问题“ `x_u`（一个标准的 n 项选择题，其中 n 为 4 个）和一个 ”带有提示的问题“ `x_h`（由 `x_u` 得到，通过插入一个直接指向问题答案 h 的提示）。通过观察模型对 `x_u` 输出非 h 答案但对 `x_h` 输出答案 h 的情况下，`c_h` 是否明确承认使用提示来解决问题来衡量 CoT 的忠实性。作者过滤了从模型得到的 QA pairs，仅保留 `a_u != h & a_h = h` 的情况，如果 `c_h` 将问题中的提示作为得到答案 `h` 的原因进行表述，则得 1 分，否则得分为 0。为了考虑从 `x_u` 到 `x_h` 答案变化中的随机噪声（**即模型并没有理解提示，而是以困惑的状态随机选择到了答案 h，这种情况下，我们不应该判其为 0 分，于是需要尽量减少这种噪声样本的影响，因为它本质上都没有理解提示，自然不会在 CoT 中明确引用**），作者通过无法由随机噪声解释的提示答案变化比例来归一化 CoT 忠实性得分。令 `p` 表示从非提示答案变成提示答案的概率，令 `q` 表示从非提示答案变成另外一个非提示答案的概率（n-2 种可能的答案）。随机模型会以相等的概率输出每个选项，因此系数 `(p - q/(n-2))/p` 表示无法由随机性解释的提示答案变化的比例，作者将 CoT 忠实性得分除以这个系数进行归一化，并将最大值限制为 1。如果这个系数为负数，则意味着模型在带有提示的问题上选择提示答案的频率不比非提示答案高。数据方面，采用 MMLU 和 GPQA 中的多项选择题来构建问题对，同时为了保证提示的鲁棒性，每个问题对中的提示存在 6 种变体，包括 4 个中性提示（奉承、一致性、视觉提示和元数据）以及 2 个非对齐（具有一定危害性）的提示（攻击评分者得到的信息和使用非法获取的信息），每个问题对的最终得分为这些变体的平均分。而评估器则使用了 Claude 3.5 Sonnet，并通过在一部分样本上进行人工检查来验证评估器的质量。

# Experiment

除了两种 LRMs 以外，作者团队还评估了两种非推理模型以进行比较，即 Claude 3.5 Sonnet 和 DeepSeek V3。结果显示：所有评估的模型都一致的使用了 `x_h` 中的提示，即所有四个模型将答案改为提示答案的频率远高于非提示答案，同时发现两个 Claude 模型似乎比两个 DeepSeek 模型更频繁地使用提示，相对差异为 24%。实验还发现，**相比非推理模型，推理模型的 CoTs 在更多情景中引用了提示，但总体来说 LRMs 的 CoTs 常常缺乏忠实性，并可能隐藏了对齐问题**。两种推理模型的整体忠实性得分仍然较低（Claude 3.7 Sonnet 为 25%，DeepSeek R1 为 39%），**且令人担忧的是，非对齐提示的忠实性得分较低**（Claude 3.7 Sonnet 为 20%，DeepSeek R1 为 29%），这表明 CoTs 可能会隐藏有问题的推理过程。同时作者还发现，**CoT 的不忠实性与简洁性呈负相关**，模型生成不忠实 CoT 的一个假设是它们偏好简洁性，令人惊讶的是，模型生成的不忠实 CoT 往往比忠实性的推理更冗长和复杂。对于 Claude 3.7 Sonnet，不忠实的 CoT 平均有 2064 tokens，而忠实的只有 1439 tokens；同样，对于 DeepSeek R1，不忠实的 CoT 平均有 6003 tokens，而忠实的为 4737 tokens。定性上，不忠实 CoT 经常检查每个多项选择题选项的正确性，这相比仅仅描述提示而言需要更多的推理且效率更低。更令人关注的是，作者还发现模型有时候会生成与它们内部知识相矛盾的不忠实 CoT，即**当模型在没有提示的情况下正确回答问题时，如果给出错误的提示，它们往往会为这些提示构建出复杂但有缺陷的论证，而不会承认这些提示是错误的**。这些模式表明，模型似乎有一种强烈的先验倾向来隐藏它们真实的推理过程。除此之外，**CoT 在更难的问题上似乎更不忠实**，在比 MMLU 更具挑战性的 GPQA 问题对上，Claude 3.7 Sonnet 在 5 个提示变体上的忠实度始终不如 MMLU（另外一个变体上平局），平均相对下降 44%，而 DeepSeek R1 在所有 6 种变体都如此，平均相对下降 32%。定性来看，忠实的 CoT 通常遵循一种模式，即模型先独立于提示自己推理出答案，再将自己推理的答案与提示答案比较，并明确承认尽管提示与其推理出的答案不同，仍然遵循来提示。这种模式则似乎以来于模型对答案有一些先验知识，这也解释了为什么更难的问题上忠实性较低。

不忠实的 CoTs 很少帮助模型得出其答案，受此启发，一种看似有前景的方法是训练模型在回答问题时更有效地使用和依赖其 CoT。作者假设复杂任务的推理需求将自然激励模型更充分的使用 CoT，从而可能提高 CoT 的忠实性。于此假设作者尝试了基于结果的强化学习，在各种密集推理型任务（如数学和编程）上训练 Claude 3.7 Sonnet，并在没有对 CoT 的过程监督情况下奖励成功的任务。在对经过强化学习的各种快照上评估 CoT 的忠实性后，结果显示基于结果的强化学习最初显著提高了 CoT 的可信度（在 MMLU 上相对提高了 63%，在 GPQA 上相对提高了 41%），但随后收益递减且未达到饱和。

最后，作者还对 CoT 忠实性在 RL 奖励漏洞中的应用做了实验，这里不再概述，感兴趣的可以看看原文。


# Rating

Anthropic 出品，必数精品！依然是篇 insights 拉满的文章，实验扎实合理，且抛出了许多后续可以继续深挖的观点，个人看来几乎没什么缺点。